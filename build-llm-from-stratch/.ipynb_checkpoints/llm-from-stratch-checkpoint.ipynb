{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9605eb-4d29-4356-bc3d-8d4d81016731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Read DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96da3aec-190f-4b26-9803-4fc29ae2b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"data-sets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "002fa49d-5fd8-4526-abff-fea83c8e714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex: use re library to example to split text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c33d10ea-77fb-42fb-965c-fbc7056617a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello,', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'simple', ' ', 'text', ' ', ':)', ' ', '.', ' ', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text=\"hello, this is a simple text :) . \"\n",
    "result_of_simple_tokenazation = re.split(r'(\\s)', text)\n",
    "print(result_of_simple_tokenazation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c41e7d1c-25f1-40ac-871a-92d6d5b6c47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nREMOVING WHITESPACES OR NOT\\n\\nWhen developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements.\\nHowever, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces.\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tip for tokenization\n",
    "\n",
    "\"\"\"\n",
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements.\n",
    "However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de0abeee-c43d-46ff-ab89-7ab308702702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Create TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f15bc30-f9bd-49ea-8888-707e07a6864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()] #remove whitespace's\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bef9f7a-d462-4265-bfb7-26b7d77e938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed)) # our total token lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b199ebff-3d2b-4a17-a2ef-0f944eb5099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert tokens into Token Id's, Create Token Id's\n",
    "# Each unique token is mapped to an unique integer called token Id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "495681c2-bf1c-443a-ab3a-c37cf53251f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed)) # get unique words with alfapeticall\n",
    "vocabulary_size = len(all_words)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "adf23295-e9e4-4dbc-8724-5af90fed3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { # assign token to unique id's\n",
    "    token: integer for integer, token in enumerate(all_words)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "846c7364-df32-4083-9a30-fd62d057854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()): # show to tokens and token Id's\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1066d2ba-038e-4f83-a98d-b4d6d70db55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: create simple Tokenizer\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab): #vocab is hash map to key value pair. # s=token, i=tokenId\n",
    "        self.str_to_int = vocab # token to tokenId need for encoder\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} # reverse is need to for decoder, tokenId to token\n",
    "    \n",
    "    def encode(self, text): # encode ile convert sample text into TOKEN Id's\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # split into individual tokens.\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() #whitespace kaldırıyor.\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) #TokenId den Token'lara dönüşüm sağlıyor.\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "54d0f795-3542-4e9c-9f7f-4aeffe0dd0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60bbffad-f936-4ff5-ae3a-0516334c4552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "467288a9-4bea-451a-b786-9b22b7b52221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will add, adding special context token, yani girilen kelime eğer datasetde yoksa error vermesin diye endoftext token ile unknown token\n",
    "#ekleyeceğiz.\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cd3fb478-6c1e-46fe-acaa-6f2dbf525f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5644c3d3-2d61-4bac-b89c-4e454a931979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab) # new tokenizer to handle new unknown token and endoftext token\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1a332e67-6c5e-4ded-a17b-c14c805383ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e67c6bf-cd3a-4229-86ce-19b51bd7825a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537562ec-a20c-4836-b39c-f0ba19098de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
